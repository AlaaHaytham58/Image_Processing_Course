{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab - Classification\n",
    "\n",
    "In this lab, we are going to build a classification module. When given an image of a handwritten digit like the one below, the model will be able to tell which digit is in the image.\n",
    "\n",
    "<img src='test2.jpg'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01margparse\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrandom\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier  # MLP is an NN\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils  # If you are unable to install this library, ask the TA; we only need this in extract_hsv_histogram.\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "# Depending on library versions on your system, one of the following imports \n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_dataset = r'digits_dataset'\n",
    "target_img_size = (32, 32) # fix image size because classification algorithms THAT WE WILL USE HERE expect that\n",
    "\n",
    "# We are going to fix the random seed to make our experiments reproducible \n",
    "# since some algorithms use pseudorandom generators\n",
    "random_seed = 42  \n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I - Feature Extraction\n",
    "\n",
    "In this part, we are going to implement three functions. Each one will extract a different set of features from the image. The three sets are:\n",
    "\n",
    "1. Histogram of the pixel values features (this is the histogram you know, but on the HSV channels)\n",
    "2. Histogram of Gradients (HoG) features\n",
    "3. Raw pixels (basically, not doing any feature extraction and just supplying the input image to the classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_hsv_histogram(img, target_img_size=(128, 128)):\n",
    "    \"\"\"\n",
    "    1. Resize image\n",
    "    2. Convert BGR â†’ HSV\n",
    "    3. Compute 3D HSV histogram with bins (8, 8, 8)\n",
    "    \"\"\"\n",
    "    resized_image = cv2.resize(img, target_img_size)\n",
    "    hsv_img = cv2.cvtColor(resized_image, cv2.COLOR_BGR2HSV)\n",
    "    hist = cv2.calcHist(\n",
    "        [hsv_img],           \n",
    "        [0, 1, 2],           \n",
    "        None,                \n",
    "        [8, 8, 8],           \n",
    "        [0, 180, 0, 256, 0, 256]   \n",
    "    ) \n",
    "\n",
    "    if imutils.is_cv2():\n",
    "        hist = cv2.normalize(hist)\n",
    "    else:\n",
    "        cv2.normalize(hist, hist)\n",
    "\n",
    "    return hist.flatten()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hog_features(img):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    You won't implement anything in this function. You just need to understand it \n",
    "    and understand its parameters (i.e win_size, cell_size, ... etc)\n",
    "    \"\"\"\n",
    "    img = cv2.resize(img, target_img_size)\n",
    "    win_size = (32, 32)\n",
    "    cell_size = (4, 4)\n",
    "    block_size_in_cells = (2, 2)\n",
    "    \n",
    "    block_size = (block_size_in_cells[1] * cell_size[1], block_size_in_cells[0] * cell_size[0])\n",
    "    block_stride = (cell_size[1], cell_size[0])\n",
    "    nbins = 9  # Number of orientation bins\n",
    "    hog = cv2.HOGDescriptor(win_size, block_size, block_stride, cell_size, nbins)\n",
    "    h = hog.compute(img)\n",
    "    h = h.flatten()\n",
    "    return h.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_raw_pixels(img):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    The classification algorithms we are going to use expect the input to be a vector not a matrix. \n",
    "    This is because they are general purpose and don't work only on images.\n",
    "    CNNs, on the other hand, expect matrices since they operate on images and exploit the \n",
    "    arrangement of pixels in the 2-D space.\n",
    "    \n",
    "    So, what we only need to do in this function is to resize and flatten the image.\n",
    "    \"\"\"\n",
    "    img = cv2.resize(img, target_img_size)\n",
    "    flattened = img.flatten()\n",
    "    \n",
    "    return flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(img, feature_set='hog'):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    Given either 'hsv_hist', 'hog', 'raw', call the respective function and return its output\n",
    "    \"\"\"\n",
    "    if feature_set == 'hsv_hist':\n",
    "        return extract_hsv_histogram(img)\n",
    "    \n",
    "    elif feature_set == 'hog':\n",
    "        return extract_hog_features(img)\n",
    "    \n",
    "    elif feature_set == 'raw':\n",
    "        return extract_raw_pixels(img)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Invalid feature_set. Choose from: 'hsv_hist', 'hog', 'raw'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function will extract the features and the label of each image in our dataset and save it in RAM. We normally don't save datasets in RAM, but this dataset is small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(feature_set='hog'):\n",
    "    features = []\n",
    "    labels = []\n",
    "    img_filenames = os.listdir(path_to_dataset)\n",
    "\n",
    "    for i, fn in enumerate(img_filenames):\n",
    "        if fn.split('.')[-1] != 'jpg':\n",
    "            continue\n",
    "\n",
    "        label = fn.split('.')[0]\n",
    "        labels.append(label)\n",
    "\n",
    "        path = os.path.join(path_to_dataset, fn)\n",
    "        img = cv2.imread(path)\n",
    "        features.append(extract_features(img, feature_set))\n",
    "        \n",
    "        # show an update every 1,000 images\n",
    "        if i > 0 and i % 1000 == 0:\n",
    "            print(\"[INFO] processed {}/{}\".format(i, len(img_filenames)))\n",
    "        \n",
    "    return features, labels        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II - Classification\n",
    "\n",
    "In this part, we will test the classification performance of SVM, KNN, & NNs given our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO understand the hyperparameters of each classifier\n",
    "classifiers = {\n",
    "    'SVM': svm.LinearSVC(random_state=random_seed),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=7),\n",
    "    'NN': MLPClassifier(solver='sgd', random_state=random_seed, hidden_layer_sizes=(500,), max_iter=20, verbose=1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will test all our classifiers on a specific feature set\n",
    "def run_experiment(feature_set):\n",
    "    \n",
    "    # Load dataset with extracted features\n",
    "    print('Loading dataset. This will take time ...')\n",
    "    features, labels = load_dataset(feature_set)\n",
    "    print('Finished loading dataset.')\n",
    "    \n",
    "    # Since we don't want to know the performance of our classifier on images it has seen before\n",
    "    # we are going to withhold some images that we will test the classifier on after training \n",
    "    train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "        features, labels, test_size=0.2, random_state=random_seed)\n",
    "    \n",
    "    for model_name, model in classifiers.items():\n",
    "        print('############## Training', model_name, \"##############\")\n",
    "        # Train the model only on the training features\n",
    "        model.fit(train_features, train_labels)\n",
    "        \n",
    "        # Test the model on images it hasn't seen before\n",
    "        accuracy = model.score(test_features, test_labels)\n",
    "        \n",
    "        print(model_name, 'accuracy:', accuracy*100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we see how each classifier and each feature set performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment('hog')\n",
    "\"\"\"\n",
    "You should get the following test accuracies the first time \n",
    "\n",
    "SVM accuracy ~ 97.70833333333333\n",
    "KNN accuracy ~ 96.52777777777779\n",
    "NN accuracy ~ 93.95833333333333\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment('hsv_hist')\n",
    "\"\"\"\n",
    "You should get the following test accuracies the first time \n",
    "\n",
    "SVM accuracy ~ 32.083333333333336\n",
    "KNN accuracy ~ 32.708333333333336\n",
    "NN accuracy ~ 9.722222222222223\n",
    "\"\"\"\n",
    "\n",
    "# Why low accuracies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment('raw')\n",
    "\"\"\"\n",
    "You should get the following test accuracies the first time \n",
    "\n",
    "SVM accuracy ~ 85.06944444444444\n",
    "KNN accuracy ~ 93.95833333333333\n",
    "NN accuracy ~ 88.68055555555556\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifiers list now has models trained on the last feature set you ran an experiment on. You can play around with it checking the probability it gives to each label, given an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "test_img_path = r'test2.jpg'\n",
    "img = cv2.imread(test_img_path)\n",
    "features = extract_features(img, 'raw')  # be careful of the choice of feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = classifiers['NN']\n",
    "nn.predict_proba([features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to get a better accuracy by changing the model hyperparameters and retraining."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
